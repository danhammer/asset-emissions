{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NewCo Hourly Urban Emissions Data - Dataset Cleaning\n",
    "@mattyarri\n",
    "\n",
    "**Date:** Feb 2, 2022\n",
    "\n",
    "**Goal:** We now have hourly urban emissions data, by census block, across a 8 broad sectors of the economy. We want to create some exploratory views of this data, to support an upcoming meeting between the NewCo team and Jane Lubchenko on Feburary 10th. Some things we want to be able to do in this notebook:\n",
    "* Query the hourly dataset, create new columns showing aggregations for hourly, weekly, and annual emissions\n",
    "* Merge the dataset, which is at the census block level, with data from the ACS, to enable per-capita emissions\n",
    "* Show aggregations at the urban area and state level\n",
    "* Plot data on a map across the US\n",
    "* Prototype widgets (ie dropdown menus) allowing user to quickly toggle between states, urban areas, etc\n",
    "* Write function to show \"similar cities\" based on emission profiles. This includes things like total aggregate emissions for a given sector, but also things like emissions composition (e.g. 30% come from residential)\n",
    "\n",
    "**Notebook Comment:** Due to the large file sizes involved, this notebook purely handles dataset cleaning and aggregation. A separate notebook file handles all of the visualization schemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import geopandas as gpd\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import numpy as np\n",
    "import gc\n",
    "import math\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "pio.templates.default = \"none\"\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "\n",
    "# engine = sqlalchemy.create_engine(\n",
    "#     sqlalchemy.engine.url.URL(\n",
    "#         drivername=\"postgresql\",\n",
    "#         username=\"postgres\",\n",
    "#         password=\"UOYo0U7Bz7hl79oABhLP\",\n",
    "#         host=\"vulcan.c1sk5ntzwqee.us-east-1.rds.amazonaws.com\",\n",
    "#         port=\"5432\",\n",
    "#         database=\"vulcan\",\n",
    "#     ),\n",
    "#     echo_pool=True,\n",
    "# )\n",
    "# print(\"connecting with engine \" + str(engine))\n",
    "# connection = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's read in raw hourly emissions dataset\n",
    "vulcan_df = pd.read_csv('raw_data_not_on_github/total_2015_hourly.csv')\n",
    "vulcan_df.head()\n",
    "vulcan_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Explore (Without Population Data)\n",
    "First let's start making some dashboard views using just the raw data provided by the Vulcan team, without trying to merge yet any population or economic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change column naming to drop H's from dataset\n",
    "def rename_hvars(vname):\n",
    "    try:\n",
    "        hstring, hour = vname.split('h')\n",
    "        return int(hour)\n",
    "    except:\n",
    "        return vname\n",
    "vulcan_df = vulcan_df.rename(rename_hvars,axis='columns') #Note: Will break if any other column has \"h\" in its name\n",
    "vulcan_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's do a quick check for null counts here by row. We'll just sample two columns out of the hourly values for now\n",
    "vulcan_df[['GEOID10','Sector',1,8760]].isna().sum()\n",
    "\n",
    "vulcan_df.shape\n",
    "\n",
    "printmd('#### So an important note here: About 45% of row entries here are null values. Lets do a little more digging here, but definitely a question for Geoff.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's just check if null values for sensible sectors (airports, marine vehicles, etc)\n",
    "t_df = vulcan_df[['Sector',1]]\n",
    "t_df['null_bool'] = t_df[1].isna()\n",
    "t_df['row_count'] = 1\n",
    "t_df = t_df.groupby('Sector').sum().reset_index()\n",
    "t_df['pct_null'] = t_df['null_bool'] / t_df['row_count']\n",
    "t_df\n",
    "\n",
    "printmd('#### A lot of these sectors have high percentages of null values. Marine vehicles makes sense, \\\n",
    "        but cement, nonroad, and electricity production seem very suspect.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ok, the next thing we want to do is create 5 dataframes: an hourly, daily, weekly monthly, and yearly dataframe for emissions\n",
    "#Note: There's a more elegant way to do this, but the elegant way broke memory constraints unfortunately\n",
    "#Note: It is very important that we do date logic here BEFORE we try to do any dataframe melts. Otherwise we explode memory constraints\n",
    "vulcan_df.shape\n",
    "\n",
    "def create_date(h):\n",
    "    start = datetime.datetime(2015, 1, 1)\n",
    "    return start + datetime.timedelta(hours=h-1) #Rezero hourly integer column\n",
    "\n",
    "print('Converting hourly column headers to dates...')\n",
    "t_df = vulcan_df.rename(lambda x: create_date(x) if isinstance(x,int) else x,axis='columns')\n",
    "\n",
    "print('Melting dataframe to long format...')\n",
    "long_df = t_df.melt(id_vars = ['GEOID10','Sector'],\n",
    "                    var_name = 'Hour',\n",
    "                    value_name = 'tC')\n",
    "\n",
    "print('Converting hours to days...')\n",
    "long_df['Day'] = long_df.Hour.dt.to_period('D')\n",
    "print('Converting hours to weeks...')\n",
    "long_df['Week'] = long_df.Hour.dt.to_period('W').apply(lambda r: r.start_time)\n",
    "print('Converting hours to months...')\n",
    "long_df['Month'] = long_df.Hour.dt.to_period('M')\n",
    "print('Converting hours to years...')\n",
    "long_df['Year'] = long_df.Hour.dt.to_period('Y')\n",
    "\n",
    "print('Reordering columns...')\n",
    "#Re-order columsn for cosmetics, write to output file just to save progress\n",
    "long_df = long_df[['GEOID10','Sector','Hour','Day','Week','Month','Year','tC']]\n",
    "print('Writing dataset to disk...')\n",
    "long_df.to_pickle('output_data/vulcan_unaggregated_datetime_dataset.pkl')\n",
    "long_df.sample(5)\n",
    "long_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was pretty memory intensive, so we're doing to do a little housekeeping here\n",
    "try:\n",
    "    del long_df\n",
    "except: print('No long_df')\n",
    "try:\n",
    "    del t_df\n",
    "except: print('No t_df')\n",
    "try:\n",
    "    del vulcan_df\n",
    "except: print('No vulcan_df')\n",
    "gc.collect()\n",
    "long_df = pd.read_pickle('output_data/vulcan_unaggregated_datetime_dataset.pkl')\n",
    "long_df.dtypes\n",
    "long_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next we want to get some aggregations together of all of these. Again, want to see how quickly these go\n",
    "print('Creating Hourly Dataframe')\n",
    "long_df_hour = long_df[['GEOID10','Sector','Hour','tC']]\n",
    "print('Creating Daily Dataframe')\n",
    "long_df_day = long_df[['GEOID10','Sector','Day','tC']].groupby(['GEOID10','Sector','Day']).sum().reset_index()\n",
    "print('Creating Weekly Dataframe')\n",
    "long_df_week = long_df[['GEOID10','Sector','Week','tC']].groupby(['GEOID10','Sector','Week']).sum().reset_index()\n",
    "print('Creating Monthly Dataframe')\n",
    "long_df_month = long_df[['GEOID10','Sector','Month','tC']].groupby(['GEOID10','Sector','Month']).sum().reset_index()\n",
    "print('Creating Yearly Dataframe')\n",
    "long_df_year = long_df[['GEOID10','Sector','Year','tC']].groupby(['GEOID10','Sector','Year']).sum().reset_index()\n",
    "\n",
    "long_df_hour.head()\n",
    "long_df_day.head()\n",
    "long_df_week.head()\n",
    "long_df_month.head()\n",
    "long_df_year.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write these new datasets to hard disk\n",
    "long_df_hour.to_pickle('output_data/hour_tc_dataset.pkl')\n",
    "long_df_day.to_pickle('output_data/day_tc_dataset.pkl')\n",
    "long_df_week.to_pickle('output_data/week_tc_dataset.pkl')\n",
    "long_df_month.to_pickle('output_data/month_tc_dataset.pkl')\n",
    "long_df_year.to_pickle('output_data/year_tc_dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df_hour = pd.read_pickle('output_data/hour_tc_dataset.pkl')\n",
    "long_df_day = pd.read_pickle('output_data/day_tc_dataset.pkl')\n",
    "long_df_week = pd.read_pickle('output_data/week_tc_dataset.pkl')\n",
    "long_df_month = pd.read_pickle('output_data/month_tc_dataset.pkl')\n",
    "long_df_year = pd.read_pickle('output_data/year_tc_dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in shapefile dataset\n",
    "shape_df = gpd.read_file('raw_data_not_on_github/cb_2018_us_ua10_500k/cb_2018_us_ua10_500k.shp')\n",
    "shape_df.head()\n",
    "shape_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert GEOID10 to integer within shapefile dataset to handle leading zeros\n",
    "shape_df['GEOID10_int'] = shape_df.GEOID10.apply(int)\n",
    "\n",
    "hour_df = long_df_hour.merge(shape_df[['GEOID10_int','NAME10','ALAND10','AWATER10','geometry']],\n",
    "                           how = 'left',\n",
    "                           left_on = 'GEOID10',\n",
    "                           right_on = 'GEOID10_int'\n",
    "                           )\n",
    "hour_df.head()\n",
    "\n",
    "day_df = long_df_day.merge(shape_df[['GEOID10_int','NAME10','ALAND10','AWATER10','geometry']],\n",
    "                           how = 'left',\n",
    "                           left_on = 'GEOID10',\n",
    "                           right_on = 'GEOID10_int'\n",
    "                           )\n",
    "day_df.head()\n",
    "\n",
    "week_df = long_df_week.merge(shape_df[['GEOID10_int','NAME10','ALAND10','AWATER10','geometry']],\n",
    "                           how = 'left',\n",
    "                           left_on = 'GEOID10',\n",
    "                           right_on = 'GEOID10_int'\n",
    "                           )\n",
    "week_df.head()\n",
    "\n",
    "month_df = long_df_month.merge(shape_df[['GEOID10_int','NAME10','ALAND10','AWATER10','geometry']],\n",
    "                           how = 'left',\n",
    "                           left_on = 'GEOID10',\n",
    "                           right_on = 'GEOID10_int'\n",
    "                           )\n",
    "month_df.head()\n",
    "\n",
    "year_df = long_df_year.merge(shape_df[['GEOID10_int','NAME10','ALAND10','AWATER10','geometry']],\n",
    "                           how = 'left',\n",
    "                           left_on = 'GEOID10',\n",
    "                           right_on = 'GEOID10_int'\n",
    "                           )\n",
    "year_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next we want to just clean these up a little bit in both ordering and column naming\n",
    "\n",
    "def quick_df_cleanup(df,period_id):\n",
    "    df = df.rename(columns = {'NAME10':'Urban Area','ALAND10':'Land Area','AWATER10':'Water Area'})\n",
    "    df = df[['GEOID10','Sector','Urban Area','Land Area','Water Area','geometry',period_id,'tC']]\n",
    "    return df\n",
    "\n",
    "hour_geo = quick_df_cleanup(hour_df,'Hour')\n",
    "day_geo = quick_df_cleanup(day_df,'Day')\n",
    "week_geo = quick_df_cleanup(week_df,'Week')\n",
    "month_geo = quick_df_cleanup(month_df,'Month')\n",
    "year_geo = quick_df_cleanup(year_df,'Year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write these to disk as well\n",
    "hour_geo.to_pickle('output_data/hour_geo.pkl')\n",
    "day_geo.to_pickle('output_data/day_geo.pkl')\n",
    "week_geo.to_pickle('output_data/week_geo.pkl')\n",
    "month_geo.to_pickle('output_data/month_geo.pkl')\n",
    "year_geo.to_pickle('output_data/year_geo.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduced Shapefile Merge with Better Memory Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# long_df_hour = pd.read_pickle('output_data/hour_tc_dataset.pkl')\n",
    "# long_df_day = pd.read_pickle('output_data/day_tc_dataset.pkl')\n",
    "# long_df_week = pd.read_pickle('output_data/week_tc_dataset.pkl')\n",
    "# long_df_month = pd.read_pickle('output_data/month_tc_dataset.pkl')\n",
    "long_df_year = pd.read_pickle('output_data/year_tc_dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in shapefile dataset\n",
    "shape_df = gpd.read_file('raw_data_not_on_github/cb_2018_us_ua10_500k/cb_2018_us_ua10_500k.shp')\n",
    "shape_df.head()\n",
    "shape_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert GEOID10 to integer within shapefile dataset to handle leading zeros\n",
    "shape_df['GEOID10_int'] = shape_df.GEOID10.apply(int)\n",
    "\n",
    "period_list = ['Year','Month','Week','Day','Hour']\n",
    "\n",
    "def quick_df_cleanup(df,period_id):\n",
    "    df = df.rename(columns = {'NAME10':'Urban Area','ALAND10':'Land Area','AWATER10':'Water Area'})\n",
    "    df = df[['GEOID10','Sector','Urban Area','Land Area','Water Area','geometry',period_id,'tC']]\n",
    "    return df\n",
    "\n",
    "for p in period_list:\n",
    "    print('Starting '+ p)\n",
    "    p_lower = p.lower()\n",
    "    long_df = pd.read_pickle('output_data/'+p_lower+'_tc_dataset.pkl')\n",
    "    t_df = long_df.merge(shape_df[['GEOID10_int','NAME10','ALAND10','AWATER10','geometry']],\n",
    "                           how = 'left',\n",
    "                           left_on = 'GEOID10',\n",
    "                           right_on = 'GEOID10_int'\n",
    "                           )\n",
    "    t_df.head()\n",
    "    quick_df_cleanup(t_df,p).to_pickle('output_data/'+p_lower+'_geo.pkl')\n",
    "    print('Finished writing file for '+p)\n",
    "    del long_df\n",
    "    del t_df\n",
    "    gc.collect()\n",
    "    print('Finished loop for '+p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
